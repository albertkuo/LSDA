{"nbformat_minor": 0, "cells": [{"source": "###Assignment 4 Beer Reviews\n####Allison Hwang and Albert Kuo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import numpy as np\nimport json \nimport matplotlib.pyplot as plt\nimport random\nimport re\n%matplotlib inline\nfrom pyspark.mllib.feature import HashingTF, IDF, Normalizer\nfrom pyspark.mllib.regression import LabeledPoint", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark.mllib.tree import DecisionTree, RandomForest, \\\n                                      GradientBoostedTrees", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "###part (a) Generating features (hashed TF-IDF)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "all_reviews = sc.textFile(\"s3n://stat-37601/ratings.json\",minPartitions=1000).map(json.loads)\nreviews, reviews_test = all_reviews.randomSplit([.7, .3])\nreviews.cache()\n\nseed = random.randint(0,10000)\nreviews_cv = reviews.sample(False, 0.7, seed)", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 18, "cell_type": "code", "source": "def getLabel(review):\n    \"\"\"\n    Get the overall rating from a review\n    \"\"\"\n    label, total = review[\"review_overall\"].split(\"/\")\n    return float(label) / float(total)\nlabels_cv = reviews_cv.map(getLabel)\nlabels_cv.first()", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {"text/plain": "0.65"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 6, "cell_type": "code", "source": "def parse(f):\n    wordlist = re.sub(\"[^\\w]\", \" \",  f).split()\n    for i in range(len(wordlist)):\n        wordlist[i] = wordlist[i].lower()\n    return wordlist\n", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 7, "cell_type": "code", "source": "reviews_wordlists_cv = reviews_cv.map(lambda x: parse(x['review_text']))", "outputs": [], "metadata": {"scrolled": true, "collapsed": false, "trusted": false}}, {"source": "I chose 100 features for speed considerations. Although there is lower error rates with more features (I tried various numFeatures values below), the speed improves enough for the observed increase in error to be worth it in this case.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "hashingTF = HashingTF(100)\ntf = hashingTF.transform(reviews_wordlists_cv)\ntf.cache()\nidf = IDF().fit(tf)\ntfidf = idf.transform(tf)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 9, "cell_type": "code", "source": "nor = Normalizer()\nnormalized_cv = nor.transform(tfidf)\nnormalized_cv.first()", "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "SparseVector(100, {3: 0.138, 8: 0.025, 15: 0.1444, 16: 0.3203, 17: 0.2554, 18: 0.1868, 24: 0.1704, 31: 0.1397, 33: 0.0893, 34: 0.0707, 37: 0.1003, 38: 0.154, 44: 0.0211, 46: 0.0881, 51: 0.1421, 57: 0.085, 62: 0.2019, 65: 0.1145, 70: 0.5044, 71: 0.1599, 72: 0.1305, 74: 0.1441, 75: 0.0869, 76: 0.0375, 78: 0.1417, 80: 0.1768, 81: 0.222, 85: 0.1395, 89: 0.241, 90: 0.1375, 95: 0.1809, 97: 0.1239})"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 10, "cell_type": "code", "source": "from pyspark.mllib.regression import LabeledPoint\ntrain_data = normalized_cv.zip(labels_cv).map(lambda (feature, label): LabeledPoint(label, feature))", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 13, "cell_type": "code", "source": "reviews_test.cache()\ntest_wordlists = reviews_test.map(lambda x: parse(x['review_text']))\ntest_tf = hashingTF.transform(test_wordlists)\ntest_tf.cache()\ntest_idf = IDF().fit(test_tf)\ntest_tfidf = test_idf.transform(test_tf)\nnormalized_test = nor.transform(test_tfidf)\na = normalized_test.first()", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 14, "cell_type": "code", "source": "test_labels = reviews_test.map(getLabel)", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "### part (b) function to compute mean-squared-error", "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "def mean_squared_error(model):\n    yhats = model.predict(normalized_test)\n    d = yhats.zip(test_labels)\n    diffs = d.map(lambda x: pow((x[0] - x[1]),2))\n    mse = diffs.reduce(lambda a,b: a+b)\n    return mse\n    ", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "###part (c) Trees", "cell_type": "markdown", "metadata": {}}, {"source": "## Decision Tree", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "m = DecisionTree.trainRegressor(train_data, {}, impurity='variance')\nmse = mean_squared_error(m)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 20, "cell_type": "code", "source": "# default settings:\n# impurity = 'variance', maxDepth = 5, numFeatures = 1000\nmse", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "21590.61583910424"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "There's no space left on device when i used maxDepth= 10, numFeatures=1000", "cell_type": "markdown", "metadata": {}}, {"source": "Since it seems that using numFeatures=1000 may not be the best way to conserve space, I will try using numFeatures = 100", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "# cv set, impurity = 'variance', maxDepth = 5, numFeatures = 100\nm = DecisionTree.trainRegressor(train_data, {}, impurity='variance')\nmse_numF100 = mean_squared_error(m)\n# avg is 0.1248", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 15, "cell_type": "code", "source": "mse_numF100", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "22317.612618724987"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "maxDepth = 7, numFeatures=100", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "m = DecisionTree.trainRegressor(train_data, {}, maxDepth=7, impurity='variance')\nmse_maxD7 = mean_squared_error(m)\nmse_maxD7", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "21942.210518037"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "maxDepth = 10, numFeatures = 100", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "m = DecisionTree.trainRegressor(train_data, {}, maxDepth=10, impurity='variance')\nmse_maxD10 = mean_squared_error(m)\nmse_maxD10", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "21569.050156969046"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Increases in max depth lower the error rate for Decision Trees. However, I ran out of space when I used maxDepth = 20, numFeatures = 100. ", "cell_type": "markdown", "metadata": {}}, {"source": "##Random Forest", "cell_type": "markdown", "metadata": {}}, {"source": "First I look at how the number of trees affects random forests. Having more trees lowers the number of errors but not by much as I greatly increase the number of trees.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "train_data.cache()\nm = RandomForest.trainRegressor(train_data, {}, 2)\nmse_RF_2trees = mean_squared_error(m)\nmse_RF_2trees", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "22453.99479241267"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 15, "cell_type": "code", "source": "m = RandomForest.trainRegressor(train_data, {}, 5)\nmse_RF_5trees = mean_squared_error(m)\nmse_RF_5trees", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "22290.344383166343"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 16, "cell_type": "code", "source": "#onethird is the default\nm = RandomForest.trainRegressor(train_data, {}, 20)\nmse_RF_20trees = mean_squared_error(m)\nmse_RF_20trees", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "22220.551270942728"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 13, "cell_type": "code", "source": "m = RandomForest.trainRegressor(train_data, {}, 40)\nmse_RF_40trees = mean_squared_error(m)\nmse_RF_40trees", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "22198.353201596092"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now I try different feature subset strategies. The default strategy of choosing one-third of the training set seems to work best in this case.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "m = RandomForest.trainRegressor(train_data, {}, 20, featureSubsetStrategy='all')\nmse_RF_all = mean_squared_error(m)\nmse_RF_all", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "22455.339936908935"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 18, "cell_type": "code", "source": "m = RandomForest.trainRegressor(train_data, {}, 20, featureSubsetStrategy='sqrt')\nmse_RF_sqrt = mean_squared_error(m)\nmse_RF_sqrt", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {"text/plain": "22575.90009851554"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "##Gradient Boosting", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {})\nGBT = mean_squared_error(m)\nGBT", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "20050.742135819157"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Default is 100 iterations but that took a long time to run; here is 10 iterations. There are definitely more errors when I use fewer iterations.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10)\nGBT_10iterate = mean_squared_error(m)\nGBT_10iterate", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "21880.91291290098"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "learning rate default is 0.1; here we set learning rate to 0.2, 0.3, 0.5, and 0.9. The error rate decreases as I increase the learning rate, except for learning rate=0.9. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.2)\nGBT_LR2 = mean_squared_error(m)\nGBT_LR2", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "21497.16044324459"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 15, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.3)\nGBT_LR3 = mean_squared_error(m)\nGBT_LR3", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "21203.667532941596"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 16, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.5)\nGBT_LR5 = mean_squared_error(m)\nGBT_LR5", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "21034.56692364598"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 17, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.9)\nGBT_LR9 = mean_squared_error(m)\nGBT_LR9", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "22530.51713044965"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Now I try different types of loss functions. Least squares error does better than the default loss function, log loss. Least absolute error does way worse.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.5, loss='leastSquaresError')\nGBT_leastSquares = mean_squared_error(m)\nGBT_leastSquares", "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "21052.285890285017"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 22, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(train_data, {}, numIterations=10, learningRate=0.5, loss='leastAbsoluteError')\nGBT_leastAbsErr = mean_squared_error(m)\nGBT_leastAbsErr", "outputs": [{"execution_count": 22, "output_type": "execute_result", "data": {"text/plain": "72211.62011648448"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###part (d) beyond basic features\nMake bigrams", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "reviews_wordlists_cv = reviews_cv.map(lambda x: parse(x['review_text']))\ndef double(wordlist):\n    pairs = []\n    for i in range(0, len(wordlist)-1):\n        pairs.append((wordlist[i], wordlist[i+1]))\n    return pairs\nbigrams = reviews_wordlists_cv.map(double)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 11, "cell_type": "code", "source": "hashingTF = HashingTF(100)\ntf = hashingTF.transform(bigrams)\ntf.cache()\nidf = IDF().fit(tf)\ntfidf = idf.transform(tf)\nnor = Normalizer()\nnormalized_cv = nor.transform(tfidf)\nnormalized_cv.first()", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "SparseVector(100, {3: 0.1075, 7: 0.1509, 8: 0.2788, 10: 0.2356, 12: 0.2277, 13: 0.1663, 15: 0.0889, 16: 0.0981, 17: 0.1807, 18: 0.0855, 20: 0.188, 23: 0.0933, 24: 0.092, 26: 0.1625, 28: 0.1111, 29: 0.1007, 31: 0.2835, 36: 0.1089, 40: 0.113, 41: 0.0993, 42: 0.0948, 43: 0.1987, 45: 0.114, 48: 0.0683, 50: 0.0986, 54: 0.1679, 56: 0.0997, 58: 0.1874, 60: 0.0999, 61: 0.0995, 65: 0.0994, 67: 0.2597, 70: 0.1926, 71: 0.2055, 72: 0.0955, 73: 0.1121, 74: 0.0821, 76: 0.0911, 81: 0.1803, 82: 0.1098, 84: 0.1052, 86: 0.0918, 87: 0.0964, 89: 0.0927, 92: 0.0959, 93: 0.0999, 95: 0.1258})"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 32, "cell_type": "code", "source": "bigram_train_data = normalized_cv.zip(labels_cv).map(lambda (feature, label): LabeledPoint(label, feature))", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###Random Forest with bigrams", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "m = RandomForest.trainRegressor(bigram_train_data, {}, 5)\nmse_RF_5trees_bigram = mean_squared_error(m)\nmse_RF_5trees_bigram", "outputs": [{"execution_count": 37, "output_type": "execute_result", "data": {"text/plain": "24410.33527282741"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "###Gradient Boosted Trees with bigrams", "cell_type": "markdown", "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "m = GradientBoostedTrees.trainRegressor(bigram_train_data, {}, numIterations=10, learningRate=0.5)\nGBT_LR5 = mean_squared_error(m)\nGBT_LR5", "outputs": [{"execution_count": 39, "output_type": "execute_result", "data": {"text/plain": "24270.383709230755"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Bigrams doesn't do much better, but it may be because I used 100 features when hashing. Perhaps I would see a greater difference if I used more features and there's more ability to differentiate.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}